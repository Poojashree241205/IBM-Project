# -*- coding: utf-8 -*-
"""EdututorAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11IpjXEzFxGrqOgXgVFWsWCZoY1DkqdKf
"""



import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model and tokenizer
model_name = "ibm-granite/granite-3.2-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def generate_response(prompt, max_length=800):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    if torch.cuda.is_available():
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "").strip()

# Gradio UI
def concept_explanation(concept):
    return generate_response(f"Explain the concept of {concept} with examples:")

def quiz_generator(topic):
    return generate_response(f"Generate 5 quiz questions on {topic} (multiple choice, true/false, short answer). Include answers at the end:")

with gr.Blocks() as app:
    gr.Markdown("## ðŸ“˜ Educational AI Assistant")

    with gr.Row():
        with gr.Column():
            concept = gr.Textbox(label="Concept")
            explain_btn = gr.Button("Explain Concept")
            output1 = gr.Textbox(label="Explanation", lines=10)
            explain_btn.click(concept_explanation, concept, output1)

        with gr.Column():
            topic = gr.Textbox(label="Quiz Topic")
            quiz_btn = gr.Button("Generate Quiz")
            output2 = gr.Textbox(label="Quiz + Answers", lines=15)
            quiz_btn.click(quiz_generator, topic, output2)

app.launch()

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


model_name = "ibm-granite/granite-3.2-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token


def generate_response(prompt, max_length=512):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True , max_length=512)

    if torch.cuda.is_available():
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature = 0.7,
            do_sample = True,
            pad_token_id=tokenizer.eos_token_id)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.replace(prompt, "").strip()
    return response


def concept_explanattion(concept):
    prompt = f"Explain the concept of {concept} in detail with example"
    return generate_response(prompt, max_length=800)

def quiz_generator(concept):
   prompt = f"Generate 5 quiz on the concept of {concept} with different quies types(multiple choice, true/flase, short answer):, Give me the answer at the end:"
   return generate_response(prompt, max_length=1200)

#create gradio
with gr.Blocks() as app:
  gr.Markdown("# Educational AI assistant")
  with gr.Tabs():
    with gr.TabItem("Concept Explanattion"):
      concept = gr.Textbox(label="Enter a Concept", placeholder="e.g., Machine Learning")
      explain_btn = gr.Button("Explain")
      explain_output = gr.Textbox(label="Explanation",lines=10)
      explain_btn.click(concept_explanattion, inputs=concept, outputs=explain_output)


    with gr.TabItem("Quiz Generator"):
      concept = gr.Textbox(label="Enter a Topic", placeholder="e.g., Machine Learning")
      explain_btn = gr.Button("Generate Quiz")
      explain_output = gr.Textbox(label="Quiz_Qeustion And Answer",lines=15)
      explain_btn.click(quiz_generator, inputs=concept, outputs=explain_output)


app.launch()

"""# Task
Create a Python script that uses a pre-trained language model from the `transformers` library to act as an educational tutor. The script should include a function to generate responses based on user input and a Gradio interface to interact with the AI tutor.

## Load a pre-trained language model

### Subtask:
Load a pre-trained language model from the `transformers` library that is suitable for generating educational content.

**Reasoning**:
Import the `pipeline` function and load a pre-trained language model suitable for text generation using the `pipeline` function.
"""

from transformers import pipeline

tutor_pipeline = pipeline("text-generation", model="EleutherAI/gpt-neo-125m")

"""## Define a function for generating responses

### Subtask:
Create a Python function that takes a user's question or input and uses the loaded language model to generate a relevant and informative response, acting as a tutor.

**Reasoning**:
Define a Python function that takes user input and uses the `tutor_pipeline` to generate a response, then extract and return the generated text.
"""

def get_tutor_response(user_input):
    """Generates a response from the AI tutor based on user input."""
    response = tutor_pipeline(user_input, max_length=100, num_return_sequences=1)
    generated_text = response[0]['generated_text']
    return generated_text

"""## Set up a gradio interface

### Subtask:
Build a simple web interface using Gradio to allow users to interact with the AI tutor by typing their questions and receiving responses.

**Reasoning**:
Import the gradio library and define the Gradio interface using gradio.Interface, setting the fn, inputs, outputs, and title parameters according to the instructions.
"""

import gradio as gr

iface = gr.Interface(
    fn=get_tutor_response,
    inputs='text',
    outputs='text',
    title="AI Educational Tutor"
)

"""## Launch the gradio app

### Subtask:
Run the Gradio application to make the AI tutor accessible through a web browser.

**Reasoning**:
Launch the Gradio interface to make the AI tutor accessible.
"""

iface.launch()

"""## Summary:

### Data Analysis Key Findings

*   A pre-trained language model suitable for text generation, specifically "EleutherAI/gpt-neo-125m," was successfully loaded using the `transformers` library.
*   A Python function `get_tutor_response` was defined to utilize the loaded model to generate responses to user text input, limiting the response length to 100 characters.
*   A Gradio interface named `iface` was set up to provide a web-based interaction for the AI tutor, featuring a text input field for user questions and a text output field for the model's responses, titled "AI Educational Tutor."
*   The Gradio interface was successfully launched, providing a public URL for access in the hosted environment.

### Insights or Next Steps

*   The current `max_length=100` might be too short for detailed educational explanations. Consider increasing `max_length` in the `get_tutor_response` function to allow for more comprehensive answers.
*   To improve the educational value, explore fine-tuning the pre-trained model on a dataset of educational question-answer pairs or incorporating retrieval-augmented generation to provide more accurate and contextually relevant information.

"""